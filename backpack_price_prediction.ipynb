{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, open the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_extra_df = pd.read_csv('training_extra.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the basic information of the data by info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "train_extra_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `info()` function, we can observe that the columns Brand, Material, Size, Laptop Compartment, Waterproof, Style, Color, and Weight Capacity (kg) contain a small number of missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = ['Compartments', 'Weight Capacity (kg)']\n",
    "object_columns = ['Brand', 'Material', 'Size', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']\n",
    "\n",
    "# Fill missing values in numeric columns with the median\n",
    "for col in num_columns:\n",
    "    test_df[col] = test_df[col].fillna(test_df[col].median())\n",
    "\n",
    "# Fill missing values in numeric columns with the median\n",
    "for col in num_columns:\n",
    "    train_df[col] = train_df[col].fillna(train_df[col].median())\n",
    "\n",
    "# Fill missing values in object columns with 'unknown'\n",
    "for col in object_columns:\n",
    "    test_df[col] = test_df[col].fillna('unknown')\n",
    "\n",
    "# Fill missing values in object columns with 'unknown'\n",
    "for col in object_columns:\n",
    "    train_df[col] = train_df[col].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the training and test sets have consistent dimensions, missing values are filled in this step. Alternatively, missing values in the training set could be dropped while filling them in the test set to maintain alignment. Additionally, the `train_extra_df` dataset is not used at this stage due to its large size, which could slow down computationâ€”especially since stacking is being applied. Next, we proceed to encode the object-type (categorical) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all object-type columns and inspect their unique() values\n",
    "for column in train_df.select_dtypes(include=['object']).columns:\n",
    "    unique_values = train_df[column].unique()\n",
    "    print(f\"Unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the values in the object-type columns, one-hot encoding can be safely applied without causing a dimensionality explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train_df, columns=object_columns, drop_first=True, dtype=int)\n",
    "test  = pd.get_dummies( test_df, columns=object_columns, drop_first=True, dtype=int)\n",
    "\n",
    "\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data preparation\n",
    "x = train.drop(columns=['Price','id'])\n",
    "y = train['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter optimization functions for the three models\n",
    "\n",
    "# LightGBM optimization function\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.05),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-2, 50.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-2, 50.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    return np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "# XGBoost optimization function\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-2, 50.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-2, 50.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    return np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "# CatBoost optimization function\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'depth': trial.suggest_int('depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.1),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 50.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'verbose': False,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    return np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "# Optimize LightGBM\n",
    "study_lgb = optuna.create_study(direction='minimize')\n",
    "study_lgb.optimize(objective_lgb, n_trials=50)\n",
    "best_lgb = study_lgb.best_params\n",
    "best_lgb['random_state'] = 42\n",
    "\n",
    "# Optimize XGBoost\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "best_xgb = study_xgb.best_params\n",
    "best_xgb['random_state'] = 42\n",
    "\n",
    "# Optimize CatBoost\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=50)\n",
    "best_cat = study_cat.best_params\n",
    "best_cat['verbose'] = False\n",
    "best_cat['random_state'] = 42\n",
    "\n",
    "# Define base models\n",
    "lgb_model = lgb.LGBMRegressor(**best_lgb)\n",
    "xgb_model = xgb.XGBRegressor(**best_xgb)\n",
    "cat_model = CatBoostRegressor(**best_cat)\n",
    "\n",
    "# Generate stacking features\n",
    "def generate_stacking_features(model, X_train, y_train, X_test, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof_train = np.zeros(X_train.shape[0])\n",
    "    oof_test = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, y_tr = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        oof_train[val_idx] = model.predict(X_val)\n",
    "        oof_test += model.predict(X_test)\n",
    "    \n",
    "    oof_test /= n_splits\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "\n",
    "# Generate Out-Of-Fold (OOF) features for each model\n",
    "lgb_oof, lgb_test = generate_stacking_features(lgb_model, X_train, y_train, X_test)\n",
    "xgb_oof, xgb_test = generate_stacking_features(xgb_model, X_train, y_train, X_test)\n",
    "cat_oof, cat_test = generate_stacking_features(cat_model, X_train, y_train, X_test)\n",
    "\n",
    "# Combine stacking features\n",
    "stacked_X_train = np.concatenate([lgb_oof, xgb_oof, cat_oof], axis=1)\n",
    "stacked_X_test = np.concatenate([lgb_test, xgb_test, cat_test], axis=1)\n",
    "\n",
    "# Train meta model\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_X_train, y_train)\n",
    "\n",
    "# Evaluate stacked model\n",
    "stacked_pred = meta_model.predict(stacked_X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, stacked_pred))\n",
    "# é¢„æµ‹ vs å®žé™…å€¼\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=y_test, y=stacked_pred)\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Predicted vs Actual - Stacked Model\")\n",
    "plt.grid(True)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.show()\n",
    "\n",
    "# æ®‹å·®åˆ†å¸ƒå›¾\n",
    "residuals = y_test - stacked_pred\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "residuals = y_test - stacked_pred\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=\"orange\")\n",
    "plt.title(\"ðŸ“‰ Residual Distribution (Stacked Model)\")\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Best parameters for LightGBM:\", best_lgb)\n",
    "print(\"Best parameters for XGBoost:\", best_xgb)\n",
    "print(\"Best parameters for CatBoost:\", best_cat)\n",
    "print(f\"RMSE of the stacked model: {rmse:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save your time from running,the output are as followed:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best parameters for LightGBM: {'max_depth': 6, 'num_leaves': 102, 'min_child_samples': 29, 'learning_rate': 0.04811541008025368, 'subsample': 0.6373168094008929, 'colsample_bytree': 0.6287351039931749, 'reg_alpha': 24.94216574124474, 'reg_lambda': 13.3381875824349, 'random_state': 42}\n",
    "Best parameters for XGBoost: {'max_depth': 4, 'learning_rate': 0.0242976335510153, 'subsample': 0.7392383751300301, 'colsample_bytree': 0.7081780053291772, 'alpha': 1.3548669745281217, 'lambda': 0.1300423102945354, 'n_estimators': 445, 'random_state': 42}\n",
    "Best parameters for CatBoost: {'depth': 4, 'learning_rate': 0.06257815809830002, 'l2_leaf_reg': 23.148679122356665, 'subsample': 0.6957264471620155, 'iterations': 410, 'verbose': False, 'random_state': 42}\n",
    "RMSE of the stacked model: 38.90021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation applies a multi-model stacking approach, combining three popular machine learning models: LightGBM, XGBoost, and CatBoost. The Optuna library is used to perform hyperparameter tuning for each of these models. The predictions from these base models are then used as features, which are fed into a linear regression model acting as the meta-model for a second-level prediction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
